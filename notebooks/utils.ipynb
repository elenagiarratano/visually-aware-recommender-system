{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import sys\n",
    "import os\n",
    "from io import StringIO, BytesIO\n",
    "import requests\n",
    "import boto3\n",
    "\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogFile():\n",
    "    loggers = set()\n",
    "\n",
    "    def __init__(self,\n",
    "                 directory,\n",
    "                 format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "                 rotating_file_handler={'max_bytes': 2000000, 'backup_count': 5}):\n",
    "        \"\"\"\n",
    "        Create the log file, set the log file features, store the log file in the given folder. \n",
    "\n",
    "            :param directory (str): Folder you want to store the log file in.\n",
    "            :param format (str): Format of the INFO message printed out on the log file.\n",
    "            :param rotating_file_handler (dict): Log back-up parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initial construct\n",
    "        name = __name__\n",
    "        self.format = format\n",
    "        self.level = logging.INFO\n",
    "        self.name = name\n",
    "\n",
    "        # Logger configuration\n",
    "        self.console_formatter = logging.Formatter(self.format)\n",
    "        self.console_logger = logging.StreamHandler(sys.stdout)\n",
    "        self.console_logger.setFormatter(self.console_formatter)\n",
    "\n",
    "        # Complete logging config\n",
    "        self.logger = logging.getLogger(name)\n",
    "        if name not in self.loggers:\n",
    "            self.loggers.add(name)\n",
    "        self.logger.setLevel(self.level)\n",
    "\n",
    "        log_file = datetime.now(timezone('GMT')).strftime(\n",
    "            '%Y-%m-%d %H:%M:%S').replace(' ', 'GMT').replace('-', '').replace(':', '') + '.log'\n",
    "        handler = RotatingFileHandler(os.path.join(directory, log_file),\n",
    "                                      mode='a',\n",
    "                                      maxBytes=rotating_file_handler['max_bytes'],\n",
    "                                      backupCount=rotating_file_handler['backup_count'])\n",
    "        handler.setLevel(self.level)\n",
    "        handler.setFormatter(self.console_formatter)\n",
    "        self.logger.addHandler(handler)\n",
    "        self.file_name = os.path.join(directory, log_file)\n",
    "\n",
    "    def get_logfile(self):\n",
    "        \"\"\"\n",
    "        Return the variable you need to refer to when updating the log file via .info('message').\n",
    "\n",
    "            :return logging.Logger: Logger variable you need to refer to when updating the log file.\n",
    "        \"\"\"\n",
    "        return self.logger, self.file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3():\n",
    "\n",
    "    def __init__(self,\n",
    "                 bucket_name,\n",
    "                 access_id,\n",
    "                 access_key):\n",
    "        self.s3 = boto3.client(\"s3\",\n",
    "                               aws_access_key_id=access_id,\n",
    "                               aws_secret_access_key=access_key)\n",
    "        self.bucket_name = bucket_name\n",
    "        self.bucket = boto3.resource(\"s3\").Bucket(bucket_name)\n",
    "\n",
    "    def bucket_content_keys(self):\n",
    "        key = []\n",
    "        for s3objs in self.bucket.objects.all():\n",
    "            key.append(s3objs.key)\n",
    "        return key\n",
    "\n",
    "    def mkdir(self,\n",
    "              directory):\n",
    "        self.s3.put_object(Bucket=self.bucket_name,\n",
    "                           Key=os.path.join(directory, \"/\"))\n",
    "\n",
    "    def read(self,\n",
    "             remote_filename,\n",
    "             local_filename):\n",
    "        self.bucket.download_file(remote_filename,\n",
    "                                  local_filename)\n",
    "\n",
    "    def write(self,\n",
    "              remote_filename,\n",
    "              local_filename):\n",
    "        self.s3.upload_file(Filename=local_filename,\n",
    "                            Bucket=self.bucket_name,\n",
    "                            Key=remote_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_displayer(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_translate(image_bytes,\n",
    "                    image_width=224,\n",
    "                    image_height=224):\n",
    "    img = np.uint8(np.asarray(Image.open(BytesIO(image_bytes)).convert(\"RGB\").resize((image_height, image_width))))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_train_validation_sample_batch(user_train_ratings,\n",
    "                                          user_validation_ratings,\n",
    "                                          item_images,\n",
    "                                          image_width=224,\n",
    "                                          image_height=224,\n",
    "                                          validation_sample_count=1000,\n",
    "                                          sample=True,\n",
    "                                          batch_size=None,\n",
    "                                          user_idx=None):\n",
    "    \"\"\"\n",
    "    validation_sample_count (int): Number of not-observed items to sample to get the validation set for each user.\n",
    "    \"\"\"\n",
    "\n",
    "    if batch_size is not None:\n",
    "        users = range(batch_size)\n",
    "    else:\n",
    "        users = user_idx\n",
    "\n",
    "    triplet_train_batch = {}\n",
    "    triplet_validation_batch = {}\n",
    "    for b in users:\n",
    "\n",
    "        # training set\n",
    "        if sample:\n",
    "            u = random.randrange(len(user_train_ratings))\n",
    "        else:\n",
    "            u = b\n",
    "        i = user_train_ratings[u][random.randrange(len(user_train_ratings[u]))][b'productid']\n",
    "        j = random.randrange(len(item_images))\n",
    "        while j in [item[b'productid'] for item in user_train_ratings[u]]:\n",
    "            j = random.randrange(len(item_images))\n",
    "\n",
    "        image_i = image_translate(item_images[i][b'imgs'], \n",
    "                                  image_width, \n",
    "                                  image_height)\n",
    "        image_j = image_translate(item_images[j][b'imgs'],\n",
    "                                  image_width, \n",
    "                                  image_height)\n",
    "        triplet_train_batch[u] = [image_i,\n",
    "                                  image_j]\n",
    "\n",
    "        # validation set\n",
    "        #print(\"actual RAM used: %.3f GB\" % (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * 10**(-9)))\n",
    "        #print(\"validation set for %.0f\" % (u))\n",
    "        i = user_validation_ratings[u][0][b'productid']\n",
    "        image_i = image_translate(item_images[i][b'imgs'],\n",
    "                                  image_width, \n",
    "                                  image_height)\n",
    "\n",
    "        reviewed_items = set()\n",
    "        for item in user_train_ratings[u]:\n",
    "            reviewed_items.add(item[b'productid'])\n",
    "        reviewed_items.add(user_validation_ratings[u][0][b'productid'])\n",
    "\n",
    "        triplet_validation_batch[u] = []\n",
    "        for j in random.sample(range(len(item_images)), validation_sample_count):\n",
    "            if j not in reviewed_items:\n",
    "                image_j = image_translate(item_images[j][b'imgs'],\n",
    "                                          image_width, \n",
    "                                          image_height)\n",
    "                triplet_validation_batch[u].append([image_i,\n",
    "                                                    image_j])\n",
    "\n",
    "    return triplet_train_batch, triplet_validation_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kernel_initializer(shape,\n",
    "#                        name=None):\n",
    "#     \"\"\"\n",
    "#     Initialize weights\n",
    "#     \"\"\"\n",
    "#     values = np.random.normal(loc=0,\n",
    "#                               scale=1e-2,\n",
    "#                               size=shape)\n",
    "#     return K.variable(values, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bias_initializer(shape,\n",
    "#                      name=None):\n",
    "#     \"\"\"\n",
    "#     Initialize bias\n",
    "#     \"\"\"\n",
    "#     values = np.random.normal(loc=0.5,\n",
    "#                               scale=1e-2,\n",
    "#                               size=shape)\n",
    "#     return K.variable(values, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function as ln(sigmoid) according to the BPR method\n",
    "# > why \"-\" before prediction_matrix?\n",
    "#   BPR wants to maximize the loss function while Keras engine minimizes it\n",
    "def softplus_loss(label_matrix, prediction_matrix):\n",
    "    return K.mean(K.softplus(-prediction_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric as AUC according to the BPR method\n",
    "#\n",
    "# Count the ratio of prediction value > 0\n",
    "# i.e., predicting positive item score > negative item score for a user\n",
    "#\n",
    "# Pay attention.\n",
    "# Do not use a plain integer as a parameter to switch,\n",
    "# instead, pass a compatible tensor (for example create it with K.zeros_like)\n",
    "def auc(label_tensor, prediction_tensor):\n",
    "    return K.mean(K.switch(prediction_tensor > K.zeros_like(prediction_tensor),\n",
    "                           K.ones_like(prediction_tensor),    # 1\n",
    "                           K.zeros_like(prediction_tensor)))  # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_index_by_name(model,\n",
    "                            layer_name):\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        if layer.name == layer_name:\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
